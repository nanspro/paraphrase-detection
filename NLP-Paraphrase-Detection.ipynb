{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "import numpy as np\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "from math import *\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "model = KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/train.data\", \"r\") as f:\n",
    "    training_data = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/test.data\", \"r\") as f:\n",
    "    test_data = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english')) \n",
    "  \n",
    "def get_sent_vec(line):\n",
    "    word_tokens = word_tokenize(line) \n",
    "\n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
    "\n",
    "    vector = [0] * 300\n",
    "    count = 0\n",
    "    \n",
    "    for word in filtered_sentence:\n",
    "        try:\n",
    "            vector += model[word]\n",
    "            count += 1\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    vector = vector / count\n",
    "    \n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_diff(l1, l2):\n",
    "    diff = []\n",
    "    distances = [\"euclidean\", \"minkowski\", \"cityblock\", \"cosine\", \"jaccard\", \"correlation\", \"chebyshev\", \"canberra\", \"braycurtis\", \"kulsinski\", \"sokalsneath\"]\n",
    "    for x in distances:\n",
    "        try:\n",
    "            diff.append(cdist(l1.reshape(1,300), l2.reshape(1,300), x))\n",
    "        except Exception as E:\n",
    "            print(E)\n",
    "\n",
    "    return diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_x_y(data, test=False):\n",
    "    X = []\n",
    "    Y = []\n",
    "\n",
    "    for i in data:\n",
    "        try:\n",
    "            lines = i.split(\"\\t\")\n",
    "\n",
    "            l1 = lines[2]\n",
    "            l2 = lines[3]\n",
    "            l1_vec = get_sent_vec(l1)\n",
    "            l2_vec = get_sent_vec(l2)\n",
    "\n",
    "            wm = model.wmdistance(l1.split(), l2.split())\n",
    "            diff_vector = get_diff(l1_vec, l2_vec)\n",
    "            diff_vector.append(wm)\n",
    "            if test:\n",
    "                label = int(lines[4])\n",
    "            else:\n",
    "                label = int(lines[4][1])\n",
    "            X.append(diff_vector)\n",
    "            Y.append(label)\n",
    "\n",
    "        except Exception as E:\n",
    "            print(E)\n",
    "\n",
    "    return np.array(X), np.array(Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unsupported operand type(s) for /: 'list' and 'int'\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train = make_x_y(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'51\\t8 Mile\\tAll the home alones watching 8 mile\\t8 mile is on thats my movie\\t3\\tAll/O/DT/B-NP/O the/O/DT/I-NP/O home/O/NN/I-NP/O alones/O/VBZ/B-VP/O watching/O/VBG/I-VP/B-EVENT 8/O/CD/B-NP/O mile/O/NN/I-NP/O\\t8/O/NN/B-NP/O mile/O/NN/I-NP/O is/O/VBZ/B-VP/O on/O/IN/B-PP/O thats/O/NNS/B-NP/O my/O/PRP$/B-NP/O movie/O/NN/I-NP/B-EVENT\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, Y_test = make_x_y(test_data, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13062, 12)\n",
      "(13062,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(972, 12)\n",
      "(972,)\n"
     ]
    }
   ],
   "source": [
    "print(X_test.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=True)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LinearRegression(normalize=True)\n",
    "\n",
    "lr.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0719194971727128"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(Y_test, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtr = DecisionTreeRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
       "           max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "           min_impurity_split=None, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           presort=False, random_state=None, splitter='best')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtr.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = dtr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5651397011046133"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(p, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=200, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20987654320987653"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(Y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import numpy\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_cat = to_categorical(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "NNmodel = Sequential()\n",
    "NNmodel.add(Dense(12, input_dim=12, activation='relu'))\n",
    "NNmodel.add(Dense(8, activation='relu'))\n",
    "NNmodel.add(Dense(6, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "NNmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13062, 12)\n",
      "(13062,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(972, 12)\n",
      "(972,)\n"
     ]
    }
   ],
   "source": [
    "print(X_test.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "13062/13062 [==============================] - 4s 326us/step - loss: 4.3870 - acc: 0.1295\n",
      "Epoch 2/20\n",
      "13062/13062 [==============================] - 1s 93us/step - loss: 4.1571 - acc: 0.1178\n",
      "Epoch 3/20\n",
      "13062/13062 [==============================] - 1s 99us/step - loss: 4.1383 - acc: 0.2364\n",
      "Epoch 4/20\n",
      "13062/13062 [==============================] - 1s 103us/step - loss: 4.0088 - acc: 0.3995\n",
      "Epoch 5/20\n",
      "13062/13062 [==============================] - 1s 83us/step - loss: 3.9924 - acc: 0.4006\n",
      "Epoch 6/20\n",
      "13062/13062 [==============================] - 1s 80us/step - loss: 3.9914 - acc: 0.4025\n",
      "Epoch 7/20\n",
      "13062/13062 [==============================] - 1s 84us/step - loss: 3.2879 - acc: 0.3979\n",
      "Epoch 8/20\n",
      "13062/13062 [==============================] - 1s 99us/step - loss: 1.5806 - acc: 0.4049\n",
      "Epoch 9/20\n",
      "13062/13062 [==============================] - 1s 94us/step - loss: 1.5758 - acc: 0.4052\n",
      "Epoch 10/20\n",
      "13062/13062 [==============================] - 1s 87us/step - loss: 1.5749 - acc: 0.4064\n",
      "Epoch 11/20\n",
      "13062/13062 [==============================] - 1s 80us/step - loss: 1.5761 - acc: 0.4080\n",
      "Epoch 12/20\n",
      "13062/13062 [==============================] - 1s 114us/step - loss: 1.5728 - acc: 0.4064\n",
      "Epoch 13/20\n",
      "13062/13062 [==============================] - 1s 108us/step - loss: 1.5714 - acc: 0.4096\n",
      "Epoch 14/20\n",
      "13062/13062 [==============================] - 1s 113us/step - loss: 1.5726 - acc: 0.4087\n",
      "Epoch 15/20\n",
      "13062/13062 [==============================] - 1s 87us/step - loss: 1.5704 - acc: 0.4096\n",
      "Epoch 16/20\n",
      "13062/13062 [==============================] - 1s 99us/step - loss: 1.5692 - acc: 0.4100\n",
      "Epoch 17/20\n",
      "13062/13062 [==============================] - 1s 100us/step - loss: 1.5696 - acc: 0.4100\n",
      "Epoch 18/20\n",
      "13062/13062 [==============================] - 1s 106us/step - loss: 1.5681 - acc: 0.4110\n",
      "Epoch 19/20\n",
      "13062/13062 [==============================] - 1s 93us/step - loss: 1.5670 - acc: 0.4117\n",
      "Epoch 20/20\n",
      "13062/13062 [==============================] - 1s 98us/step - loss: 1.5673 - acc: 0.4110\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f01f44a4cf8>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model\n",
    "NNmodel.fit(X_train, Y_train_cat, epochs=20, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_x_y_2(data, test=False):\n",
    "    x1 = []\n",
    "    x2 = []\n",
    "    Y = []\n",
    "\n",
    "    for i in data:\n",
    "        try:\n",
    "            lines = i.split(\"\\t\")\n",
    "            \n",
    "            l1 = lines[2]\n",
    "            l2 = lines[3]\n",
    "            l1s = l1.split()\n",
    "            l2s = l2.split()\n",
    "            lol.update(l1s)\n",
    "            lol.update(l2s)\n",
    "            if test:\n",
    "                l = int(lines[4])\n",
    "            else:\n",
    "                l = int(lines[4][1])\n",
    "            label = 1\n",
    "            if l < 2.5:\n",
    "                label = 0\n",
    "            x1.append(l1s)\n",
    "            x2.append(l2s)\n",
    "            Y.append(label)\n",
    "\n",
    "        except Exception as E:\n",
    "            print(E)\n",
    "\n",
    "    return x1, x2, np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "lol = set([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_1, texts_2, labels = make_x_y_2(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_texts_1, test_texts_2, test_labels = make_x_y_2(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_words = len(lol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 30\n",
    "EMBEDDING_DIM = 300\n",
    "VALIDATION_SPLIT = 0.1\n",
    "\n",
    "num_lstm = np.random.randint(175, 275)\n",
    "num_dense = np.random.randint(100, 150)\n",
    "rate_drop_lstm = 0.15 + np.random.rand() * 0.25\n",
    "rate_drop_dense = 0.15 + np.random.rand() * 0.25\n",
    "\n",
    "act = 'relu'\n",
    "re_weight = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "for i, word in enumerate(lol):\n",
    "    if word in model.vocab:\n",
    "        embedding_matrix[i] = model.word_vec(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11708, 300)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from string import punctuation\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8841 unique tokens\n",
      "Shape of data tensor: (13063, 30)\n",
      "Shape of label tensor: (13063,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenizer = Tokenizer(num_words=nb_words)\n",
    "tokenizer.fit_on_texts(texts_1 + texts_2 + test_texts_1 + test_texts_2)\n",
    "\n",
    "sequences_1 = tokenizer.texts_to_sequences(texts_1)\n",
    "sequences_2 = tokenizer.texts_to_sequences(texts_2)\n",
    "test_sequences_1 = tokenizer.texts_to_sequences(test_texts_1)\n",
    "test_sequences_2 = tokenizer.texts_to_sequences(test_texts_2)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens' % len(word_index))\n",
    "\n",
    "data_1 = pad_sequences(sequences_1, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "data_2 = pad_sequences(sequences_2, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "labels = np.array(labels)\n",
    "print('Shape of data tensor:', data_1.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "test_data_1 = pad_sequences(test_sequences_1, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "test_data_2 = pad_sequences(test_sequences_2, maxlen=MAX_SEQUENCE_LENGTH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(nb_words,\n",
    "        EMBEDDING_DIM,\n",
    "        weights=[embedding_matrix],\n",
    "        input_length=MAX_SEQUENCE_LENGTH,\n",
    "        trainable=False)\n",
    "lstm_layer = LSTM(num_lstm, dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm)\n",
    "\n",
    "sequence_1_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences_1 = embedding_layer(sequence_1_input)\n",
    "x1 = lstm_layer(embedded_sequences_1)\n",
    "\n",
    "sequence_2_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences_2 = embedding_layer(sequence_2_input)\n",
    "y1 = lstm_layer(embedded_sequences_2)\n",
    "\n",
    "merged = concatenate([x1, y1])\n",
    "merged = Dropout(rate_drop_dense)(merged)\n",
    "merged = BatchNormalization()(merged)\n",
    "\n",
    "merged = Dense(num_dense, activation=act)(merged)\n",
    "merged = Dropout(rate_drop_dense)(merged)\n",
    "merged = BatchNormalization()(merged)\n",
    "\n",
    "preds = Dense(1, activation='sigmoid')(merged)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTMmodel = Model(inputs=[sequence_1_input, sequence_2_input], \\\n",
    "        outputs=preds)\n",
    "LSTMmodel.compile(loss='binary_crossentropy',\n",
    "        optimizer='nadam',\n",
    "        metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_1_train = np.vstack((data_1, data_2))\n",
    "data_2_train = np.vstack((data_2, data_1))\n",
    "labels_train = np.concatenate((labels, labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "26126/26126 [==============================] - 270s 10ms/step - loss: 0.5763 - acc: 0.7168\n",
      "Epoch 2/10\n",
      "26126/26126 [==============================] - 255s 10ms/step - loss: 0.4995 - acc: 0.7697\n",
      "Epoch 3/10\n",
      "26126/26126 [==============================] - 254s 10ms/step - loss: 0.4706 - acc: 0.7869\n",
      "Epoch 4/10\n",
      "26126/26126 [==============================] - 263s 10ms/step - loss: 0.4456 - acc: 0.7987\n",
      "Epoch 5/10\n",
      "26126/26126 [==============================] - 282s 11ms/step - loss: 0.4211 - acc: 0.8146\n",
      "Epoch 6/10\n",
      "26126/26126 [==============================] - 284s 11ms/step - loss: 0.3984 - acc: 0.8270\n",
      "Epoch 7/10\n",
      "26126/26126 [==============================] - 271s 10ms/step - loss: 0.3740 - acc: 0.8382\n",
      "Epoch 8/10\n",
      "26126/26126 [==============================] - 266s 10ms/step - loss: 0.3604 - acc: 0.8481\n",
      "Epoch 9/10\n",
      "26126/26126 [==============================] - 260s 10ms/step - loss: 0.3442 - acc: 0.8583\n",
      "Epoch 10/10\n",
      "26126/26126 [==============================] - 250s 10ms/step - loss: 0.3318 - acc: 0.8630\n"
     ]
    }
   ],
   "source": [
    "hist = LSTMmodel.fit([data_1_train, data_2_train], labels_train, \\\n",
    "        epochs=10, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13063/13063 [==============================] - 39s 3ms/step\n",
      "13063/13063 [==============================] - 39s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "data_1_test = np.vstack((test_data_1, test_data_2))\n",
    "data_2_test = np.vstack((test_data_2, test_data_1))\n",
    "\n",
    "\n",
    "LSTMpreds = LSTMmodel.predict([test_data_1, test_data_2], batch_size=10, verbose=1)\n",
    "LSTMpreds += LSTMmodel.predict([test_data_2, test_data_1], batch_size=10, verbose=1)\n",
    "LSTMpreds /= 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9558617 ],\n",
       "       [0.94754326],\n",
       "       [0.97819096],\n",
       "       ...,\n",
       "       [0.04443993],\n",
       "       [0.0987117 ],\n",
       "       [0.0401309 ]], dtype=float32)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LSTMpreds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTMpreds = numpy.rint(LSTMpreds.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., ..., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13063,)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9203858225522468"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(test_labels, LSTMpreds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9048460656681441\n",
      "0.9114509431115994\n",
      "0.898981588845932\n"
     ]
    }
   ],
   "source": [
    "print(f1_score(test_labels, LSTMpreds, average=\"macro\"))\n",
    "print(precision_score(test_labels, LSTMpreds, average=\"macro\"))\n",
    "print(recall_score(test_labels, LSTMpreds, average=\"macro\"))    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
